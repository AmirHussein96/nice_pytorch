{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual libraries:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom modules/losses from NICE:\n",
    "from nice.models import NICEModel\n",
    "from nice.loss import logistic_nice_loglkhd, LogisticPriorNICELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. model\n",
    "\n",
    "Let's make sure that our model is capable of inverting arbitrary inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model with and without batch norm, three layers deep:\n",
    "nice_bn = NICEModel(input_dim=256, hidden_dim=64, num_layers=3, bn=True)\n",
    "nice_mlp = NICEModel(input_dim=256, hidden_dim=64, num_layers=3, bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare a tensor with its inverse-mapped value:\n",
    "def recover_tensor(tsr, mdl):\n",
    "    with torch.no_grad():\n",
    "        return torch.dist(mdl.inverse(mdl(tsr)), tsr, p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6177e-05)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this multiple times and make sure the values are small:\n",
    "recover_tensor(torch.randn(8,256), nice_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0455)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing for the non-batchnormed model:\n",
    "recover_tensor(torch.randn(8,256), nice_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. losses\n",
    "Now let's make sure that we can train against our loss module to memorize a single instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0470e-01, -3.2764e-01,  2.5212e-01,  ..., -6.7200e-01,\n",
      "         -1.0572e+00,  1.8751e-01],\n",
      "        [-3.2082e-01, -8.4130e-01,  7.4957e-01,  ..., -7.2816e-01,\n",
      "          2.1122e-01,  1.9313e+00],\n",
      "        [ 9.4984e-01,  4.1410e-01, -5.2190e-01,  ...,  2.1097e-01,\n",
      "          1.3180e-02, -1.6768e+00],\n",
      "        ...,\n",
      "        [-1.2008e-01, -1.4522e-01, -8.4159e-01,  ...,  1.0249e+00,\n",
      "         -9.0380e-01,  1.2813e-03],\n",
      "        [ 4.3895e-01,  1.0337e+00, -1.0610e+00,  ...,  9.5216e-01,\n",
      "         -9.6363e-02, -2.7394e-02],\n",
      "        [-1.1182e+00, -2.3659e+00,  2.8266e-01,  ...,  3.7139e+00,\n",
      "         -3.8203e-01, -7.4203e-01]])\n"
     ]
    }
   ],
   "source": [
    "# this will be the random tensor that we'll try to get our model to memorize; run this cell *once*.\n",
    "constant = torch.randn(8,256)\n",
    "print(constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- part 1: fitting with the batchnorm NICE model:\n",
    "loss_fn = LogisticPriorNICELoss(size_average=True)\n",
    "opt = optim.Adam(nice_bn.parameters(), lr=0.00005, betas=(0.9, 0.999), eps=1e-8)\n",
    "def train(n=1):\n",
    "    nice_bn.train()\n",
    "    for k in tqdm.trange(n):\n",
    "        opt.zero_grad()\n",
    "        (-loss_fn(nice_bn(constant), nice_bn.scaling_diag)).backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        nice_bn.eval()\n",
    "        print(-loss_fn(nice_bn(constant), nice_bn.scaling_diag).item())\n",
    "        nice_bn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253.4134979248047\n"
     ]
    }
   ],
   "source": [
    "# try out the loss function, which should compute negative log-likelihood against a logistic distribution:\n",
    "with torch.no_grad():\n",
    "    nice_bn.eval()\n",
    "    print(-loss_fn(nice_bn(constant), nice_bn.scaling_diag).item())\n",
    "    nice_bn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4070.748779296875\n"
     ]
    }
   ],
   "source": [
    "# define method to sample (elementwise) from a standard logistic:\n",
    "def sample_logistic_like(tsr):\n",
    "    Z = torch.rand_like(tsr)\n",
    "    Y = torch.log(Z) - torch.log(1-Z)\n",
    "    return Y\n",
    "with torch.no_grad():\n",
    "    print(logistic_nice_loglkhd(sample_logistic_like(constant)).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "# see if we can recover the tensor from sampling:\n",
    "with torch.no_grad():\n",
    "    print(torch.dist(constant, nice_bn.inverse(sample_logistic_like(constant)), p=2.).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- part 2: the same thing, with the standard (no-batchnorm) NICE model:\n",
    "# (N.B.: run this *after* running the above cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. training\n",
    "Finally, let's overfit against a single MNIST image and recover it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
